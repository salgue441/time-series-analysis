{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Time Series Forecasting\n",
    "\n",
    "A sophisticated ensemble forecasting system combining SARIMA and LSTM models for financial time series prediction.\n",
    "\n",
    "## ðŸ” Overview\n",
    "\n",
    "This project implements a hybrid forecasting system that combines statistical (SARIMA) and deep learning (LSTM) approaches for financial time series prediction. The system includes data preprocessing, visualization, and ensemble prediction capabilities.\n",
    "\n",
    "## âœ¨ Features\n",
    "\n",
    "- Data preprocessing and feature engineering\n",
    "- Interactive visualizations\n",
    "- SARIMA model with hyperparameter optimization\n",
    "- Bidirectional LSTM with configurable architecture\n",
    "- Ensemble forecasting\n",
    "- Comprehensive evaluation metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import multiprocessing as mp\n",
    "from itertools import product\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    ")\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import optuna\n",
    "import logging\n",
    "import abc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s: %(message)s\",\n",
    "    handlers=[logging.StreamHandler(), logging.FileHandler(\"sarima_lstm.log\")],\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    Data processor designed to fetch and process financial data from\n",
    "    yahoo financial.\n",
    "\n",
    "    Attributes:\n",
    "        tickers (List[str]): List of tickers to download the data from\n",
    "        start_date (str): Start date of the tickers\n",
    "        end_date (str): End date of the tickers\n",
    "        raw_data (dict): Raw data of the financial data\n",
    "        processed_data (dict): Processed data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tickers: List[str] = [\"AAPL\"],\n",
    "        start_date: str = \"2010-01-01\",\n",
    "        end_date: str = \"2024-01-01\",\n",
    "    ):\n",
    "        self.tickers = tickers\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.raw_data, self.processed_data = {}, {}\n",
    "\n",
    "    def fetch_market_data(self) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Fetches market data for the specified tickers\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, pd.DataFrame]: A dictionary containing the raw data for each ticker\n",
    "        \"\"\"\n",
    "\n",
    "        for ticker in tqdm(self.tickers, desc=\"Fetching data\"):\n",
    "            try:\n",
    "                data = yf.download(ticker, start=self.start_date, end=self.end_date)\n",
    "\n",
    "                if data.empty:\n",
    "                    logger.warning(f\"No data found for {ticker}\")\n",
    "                    continue\n",
    "\n",
    "                if len(data) < 100:\n",
    "                    logger.warning(\n",
    "                        f\"Limited data for {ticker}: only {len(data)} records\"\n",
    "                    )\n",
    "\n",
    "                self.raw_data[ticker] = data\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error fetching data for {ticker}: {e}\")\n",
    "\n",
    "        return self.raw_data\n",
    "\n",
    "    def preprocess_data(self, target_column: str = \"Close\") -> Dict[str, dict]:\n",
    "        \"\"\"\n",
    "        Data preprocessing pipeline with feature engineering and scaling. It applies the following transformations:\n",
    "\n",
    "        - Log returns\n",
    "        - Percentage returns\n",
    "        - Moving averages (20 and 50 days)\n",
    "        - Bollinger bands (20 days)\n",
    "        - Min-max scaling\n",
    "        - Standard scaling\n",
    "        - Metadata (volatility, skewness, kurtosis)\n",
    "\n",
    "        Args:\n",
    "            target_column (str, optional): The column to predict. Defaults to \"Close\".\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, dict]: A dictionary containing the processed data for each ticker\n",
    "        \"\"\"\n",
    "        for ticker, data in self.raw_data.items():\n",
    "            try:\n",
    "                log_returns = np.log(\n",
    "                    data[target_column] / data[target_column].shift(1)\n",
    "                ).dropna()\n",
    "                pct_returns = data[target_column].pct_change().dropna()\n",
    "\n",
    "                ma_20 = data[target_column].rolling(window=20).mean()\n",
    "                ma_50 = data[target_column].rolling(window=50).mean()\n",
    "                std_20 = data[target_column].rolling(window=20).std()\n",
    "\n",
    "                processed = {\n",
    "                    \"raw\": data[target_column],\n",
    "                    \"log_returns\": log_returns,\n",
    "                    \"pct_returns\": pct_returns,\n",
    "                    \"MA_20\": ma_20,\n",
    "                    \"MA_50\": ma_50,\n",
    "                    \"bollinger_high\": ma_20 + 2 * std_20,\n",
    "                    \"bollinger_low\": ma_20 - 2 * std_20,\n",
    "                }\n",
    "\n",
    "                scaler_minmax = MinMaxScaler()\n",
    "                scaler_standard = StandardScaler()\n",
    "\n",
    "                processed[\"scaled_minmax\"] = scaler_minmax.fit_transform(\n",
    "                    data[target_column].values.reshape(-1, 1)\n",
    "                )\n",
    "                processed[\"scaled_standard\"] = scaler_standard.fit_transform(\n",
    "                    data[target_column].values.reshape(-1, 1)\n",
    "                )\n",
    "\n",
    "                processed[\"metadata\"] = {\n",
    "                    \"volatility\": log_returns.std(),\n",
    "                    \"skewness\": log_returns.skew(),\n",
    "                    \"kurtosis\": log_returns.kurtosis(),\n",
    "                }\n",
    "\n",
    "                self.processed_data[ticker] = processed\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error preprocessing data: {e}\")\n",
    "\n",
    "        return self.processed_data\n",
    "\n",
    "    @staticmethod\n",
    "    def create_lagged_method(data: pd.Series, lags: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates lagged features for time series data\n",
    "\n",
    "        Args:\n",
    "            data (pd.Series): Time series data\n",
    "            lags (int, optional): Number of lags to create. Defaults to 5.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with lagged features\n",
    "        \"\"\"\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        for i in range(1, lags + 1):\n",
    "            df[f\"lag_{i}\"] = df.iloc[:, 0].shift(i)\n",
    "\n",
    "        return df.dropna()\n",
    "\n",
    "    @staticmethod\n",
    "    def add_rolling_features(data: pd.Series) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Add rolling window features to the data\n",
    "\n",
    "        Args:\n",
    "            data (pd.Series): Time series data\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with rolling window features\n",
    "        \"\"\"\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        windows = [7, 14, 30]\n",
    "\n",
    "        for window in windows:\n",
    "            df[f\"rolling_mean_{window}\"] = df.iloc[:, 0].rolling(window=window).mean()\n",
    "            df[f\"rolling_std_{window}\"] = df.iloc[:, 0].rolling(window=window).std()\n",
    "\n",
    "        return df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialRatiosVisualizer:\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    def __init__(self, processed_data: pd.DataFrame, raw_data: pd.DataFrame):\n",
    "        self.processed_data = processed_data\n",
    "        self.raw_data = raw_data\n",
    "\n",
    "    def plot_closing_prices(self):\n",
    "        \"\"\"\n",
    "        Visualize the closing prices of the stock\n",
    "        \"\"\"\n",
    "\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        for ticker, data in self.raw_data.items():\n",
    "            plt.plot(data.index, data[\"Close\"], label=ticker)\n",
    "\n",
    "        plt.title(\"Closing Prices Comparison\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Price\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_returns_distribution(self):\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        for ticker, data in self.raw_data.items():\n",
    "            returns = data[\"Close\"].pct_change().dropna()\n",
    "            sns.histplot(returns, kde=True, label=ticker)\n",
    "\n",
    "        plt.title(\"Returns Distribution\")\n",
    "        plt.xlabel(\"Daily Returns\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_correlation_heatmap(self):\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        correlation_matrix = pd.concat(\n",
    "            [data[\"Close\"] for data in self.raw_data.values()], axis=1\n",
    "        ).corr()\n",
    "\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\")\n",
    "        plt.title(\"Correlation Heatmap\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_rolling_volatility(self):\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        for ticker, data in self.raw_data.items():\n",
    "            returns = data[\"Close\"].pct_change().dropna()\n",
    "            rolling_std = returns.rolling(window=20).std()\n",
    "            plt.plot(data.index, rolling_std, label=ticker)\n",
    "\n",
    "        plt.title(\"Rolling Volatility Comparison\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Volatility\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def perform_statistical_test(self):\n",
    "        print(\"\\nStatistical Tests\\n\")\n",
    "        for ticker, data in self.raw_data.items():\n",
    "            returns = data[\"Close\"].pct_change().dropna()\n",
    "            adf_result = adfuller(returns)\n",
    "\n",
    "            print(f\"\\n{ticker} Augmented Dickey-Fuller Test\")\n",
    "            print(f\"ADF Statistic: {adf_result[0]}\")\n",
    "            print(f\"p-value: {adf_result[1]}\")\n",
    "\n",
    "            _, p_value = stats.normaltest(returns)\n",
    "            print(f\"Normality Test p-value: {p_value}\")\n",
    "\n",
    "    def visualize_single_stock_preprocessing(self, ticker):\n",
    "        data = self.raw_data[ticker]\n",
    "        processed = self.processed_data[ticker]\n",
    "        target_column = \"Close\"\n",
    "\n",
    "        plt.figure(figsize=(20, 10))\n",
    "\n",
    "        # Original vs Moving Averages\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.title(\"Original vs Moving Averages\")\n",
    "        plt.plot(data.index, data[target_column], label=\"Original\")\n",
    "        plt.plot(data.index, processed[\"MA_20\"], label=\"20-day MA\")\n",
    "        plt.plot(data.index, processed[\"MA_50\"], label=\"50-day MA\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Bollinger Bands\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.title(\"Bollinger Bands\")\n",
    "        plt.plot(data.index, data[target_column], label=\"Price\")\n",
    "        plt.plot(data.index, processed[\"MA_20\"], label=\"MA 20\")\n",
    "        plt.plot(data.index, processed[\"bollinger_high\"], label=\"Bollinger High\")\n",
    "        plt.plot(data.index, processed[\"bollinger_low\"], label=\"Bollinger Low\")\n",
    "        plt.legend()\n",
    "\n",
    "        # Log Returns Distribution\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.title(\"Log Returns Distribution\")\n",
    "        sns.histplot(processed[\"log_returns\"], kde=True)\n",
    "        plt.xlabel(\"Log Returns\")\n",
    "\n",
    "        # Returns Scaling Comparison\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.title(\"Returns Scaling Comparison\")\n",
    "        plt.plot(data.index, processed[\"scaled_minmax\"], label=\"MinMax Scaled\")\n",
    "        plt.plot(data.index, processed[\"scaled_standard\"], label=\"Standard Scaled\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # ACF and PACF for Autocorrelation\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plot_acf(processed[\"log_returns\"], lags=40, ax=plt.gca())\n",
    "        plt.title(\"Autocorrelation Function\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plot_pacf(processed[\"log_returns\"], lags=40, ax=plt.gca())\n",
    "        plt.title(\"Partial Autocorrelation Function\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def run_comprehensive_visualization(self):\n",
    "        self.plot_closing_prices()\n",
    "        self.plot_returns_distribution()\n",
    "        self.plot_correlation_heatmap()\n",
    "        # self.plot_rolling_volatility()\n",
    "\n",
    "        self.perform_statistical_test()\n",
    "        for ticker in self.processed_data.keys():\n",
    "            self.visualize_single_stock_preprocessing(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Forecaster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseForecaster(abc.ABC):\n",
    "    def __init__(self, config: Dict[str, Any] = None):\n",
    "        self.config = config or {}\n",
    "        self.model = None\n",
    "        self.preprocessor = None\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def preprocess(self, data: pd.Series) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Preprocess input data for forecasting\n",
    "\n",
    "        Args:\n",
    "            data (pd.Series): Input time series data\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Preprocessed data\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Train the forecasting model\n",
    "\n",
    "        Args:\n",
    "            X_train (np.ndarray): Training features data\n",
    "            y_train (np.ndarray): Training target values\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def forecast(self, steps: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate forecast for specified number of steps ahead\n",
    "\n",
    "        Args:\n",
    "            steps (int): Number of steps to forecast\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Forecasted values\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluates the model performance on test data\n",
    "\n",
    "        Args:\n",
    "            X_test (np.ndarray): Test features data\n",
    "            y_test (np.ndarray): Test target values\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Evaluation metrics\n",
    "        \"\"\"\n",
    "\n",
    "        predictions = self.model.predict(X_test)\n",
    "        return {\n",
    "            \"MSE\": mean_squared_error(y_test, predictions),\n",
    "            \"MAE\": mean_absolute_error(y_test, predictions),\n",
    "            \"MAPE\": mean_absolute_percentage_error(y_test, predictions),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMA Hyperparameter Tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterOptimizer:\n",
    "    @staticmethod\n",
    "    def optimize_sarima(data: pd.Series) -> Tuple[Dict[str, int], float]:\n",
    "        \"\"\"\n",
    "        Optimize SARIMA hyperparameters using optuna\n",
    "\n",
    "        Args:\n",
    "            data (pd.Series): Input time series data\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Dict[str, int], float]: Optimal hyperparameters and best score\n",
    "        \"\"\"\n",
    "\n",
    "        def objective(trial):\n",
    "            p = trial.suggest_int(\"p\", 0, 3)\n",
    "            d = trial.suggest_int(\"d\", 0, 2)\n",
    "            q = trial.suggest_int(\"q\", 0, 3)\n",
    "            P = trial.suggest_int(\"P\", 0, 2)\n",
    "            D = trial.suggest_int(\"D\", 0, 1)\n",
    "            Q = trial.suggest_int(\"Q\", 0, 2)\n",
    "            m = trial.suggest_categorical(\"m\", [4, 12])\n",
    "\n",
    "            try:\n",
    "                model = SARIMAX(data, order=(p, d, q), seasonal_order=(P, D, Q, m))\n",
    "\n",
    "                results = model.fit(disp=False)\n",
    "                return results.aic\n",
    "\n",
    "            except Exception as e:\n",
    "                return np.inf\n",
    "\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(objective, n_trials=100)\n",
    "\n",
    "        return study.best_params, study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMA Forecaster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARIMAForecaster(BaseForecaster):\n",
    "    def preprocess(self, data):\n",
    "        return data.values\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Train SARIMA model using the optimal hyperparameters\n",
    "\n",
    "        Args:\n",
    "            X_train (np.ndarray): Training features data\n",
    "            y_train (np.ndarray): Training target values\n",
    "\n",
    "        Returns:\n",
    "            SARIMAForecaster: Trained SARIMA model\n",
    "        \"\"\"\n",
    "\n",
    "        best_params, _ = HyperparameterOptimizer.optimize_sarima(\n",
    "            pd.Series(y_train, name=\"target\")\n",
    "        )\n",
    "\n",
    "        self.model = SARIMAX(\n",
    "            y_train,\n",
    "            order=(best_params[\"p\"], best_params[\"d\"], best_params[\"q\"]),\n",
    "            seasonal_order=(\n",
    "                best_params[\"P\"],\n",
    "                best_params[\"D\"],\n",
    "                best_params[\"Q\"],\n",
    "                best_params[\"m\"],\n",
    "            ),\n",
    "        ).fit()\n",
    "\n",
    "    def forecast(self, steps: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forecast future values using SARIMA model\n",
    "\n",
    "        Args:\n",
    "            steps (int): Number of steps to forecast\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Forecasted values\n",
    "        \"\"\"\n",
    "\n",
    "        return self.model.forecast(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int = 1,\n",
    "        hidden_size: int = 64,\n",
    "        num_layers: int = 3,\n",
    "        dropout: float = 0.3,\n",
    "    ):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the LSTM model with a linear layer\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor with shape (batch_size, seq_len, input_size)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "    def train_model(\n",
    "        self,\n",
    "        train_loader: DataLoader,\n",
    "        test_loader: DataLoader,\n",
    "        num_epochs: Optional[int] = 100,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trains the LSTM model using the training and testing DataLoader\n",
    "\n",
    "        Args:\n",
    "            train_loader (DataLoader): DataLoader object for training\n",
    "            test_loader (DataLoader): DataLoader object for testing\n",
    "            num_epochs (int, optional): Number of epochs. Defaults to 100.\n",
    "\n",
    "        Returns:\n",
    "            LSTM model: Trained LSTM model\n",
    "        \"\"\"\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        num_epochs: int = num_epochs\n",
    "\n",
    "        for epoch in tqdm(range(num_epochs), desc=\"Training model\"):\n",
    "            outputs = self.train()\n",
    "\n",
    "            for i, (inputs, labels) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                self.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    test_loss = 0.0\n",
    "                    for inputs, labels in test_loader:\n",
    "                        outputs = self(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        test_loss += loss.item()\n",
    "\n",
    "                    logger.info(\n",
    "                        f\"Epoch {epoch}/{num_epochs}, Loss: {test_loss / len(test_loader)}\"\n",
    "                    )\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForecaster(BaseForecaster):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int = 1,\n",
    "        hidden_size: int = 64,\n",
    "        num_layers: int = 3,\n",
    "    ):\n",
    "        super(LSTMForecaster, self).__init__()\n",
    "        self.model = LSTM(\n",
    "            input_size=input_size, hidden_size=hidden_size, num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        scaler = StandardScaler()\n",
    "        return scaler.fit_transform(data.reshape(-1, 1))\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Trains the model using the training data\n",
    "\n",
    "        Args:\n",
    "            X_train (np.ndarray): Training features data\n",
    "            y_train (np.ndarray): Training target values\n",
    "\n",
    "        Returns:\n",
    "            LSTMForecaster: Trained LSTM model\n",
    "        \"\"\"\n",
    "\n",
    "        X_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        y_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "\n",
    "        train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        self.model.train_model(train_loader)\n",
    "\n",
    "    def forecast(self, steps: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict future values using the LSTM model\n",
    "\n",
    "        Args:\n",
    "            steps (int): Number of steps to forecast\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Forecasted values\n",
    "        \"\"\"\n",
    "\n",
    "        last_sequence = self.model.data[-self.model.window_size :]\n",
    "        last_sequence = torch.tensor(last_sequence, dtype=torch.float32).to(device)\n",
    "\n",
    "        predictions = []\n",
    "        for _ in range(steps):\n",
    "            with torch.no_grad():\n",
    "                output = self.model(last_sequence)\n",
    "                predictions.append(output.item())\n",
    "                last_sequence = torch.cat(\n",
    "                    [last_sequence[1:], output.unsqueeze(0)], axis=0\n",
    "                )\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast Ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForecastEnsemble:\n",
    "    def __init__(self, forecasters: List[BaseForecaster]):\n",
    "        if not forecasters:\n",
    "            raise ValueError(\"Forecasters list cannot be empty\")\n",
    "\n",
    "        self.forecasters = forecasters\n",
    "        self._cached_data: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Train all forecasters in the ensemble\n",
    "\n",
    "        Args:\n",
    "            X_train (np.ndarray): Training input features\n",
    "            y_train (np.ndarray): Training target values\n",
    "        \"\"\"\n",
    "\n",
    "        if X_train is None or y_train is None:\n",
    "            raise ValueError(\"Training data is missing\")\n",
    "\n",
    "        if len(X_train) == 0 or len(y_train) == 0:\n",
    "            raise ValueError(\"Empty training data\")\n",
    "\n",
    "        if \"sarima_x\" not in self._cached_data:\n",
    "            self._cached_data[\"sarima_x\"] = X_train.reshape(-1)\n",
    "            self._cached_data[\"sarima_y\"] = y_train.reshape(-1)\n",
    "\n",
    "        for forecaster in self.forecasters:\n",
    "            try:\n",
    "                if isinstance(forecaster, SARIMAForecaster):\n",
    "                    forecaster.train(\n",
    "                        self._cached_data[\"sarima_x\"], self._cached_data[\"sarima_y\"]\n",
    "                    )\n",
    "                else:\n",
    "                    forecaster.train(X_train, y_train)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error training forecaster {type(forecaster)}: {e}\")\n",
    "                raise\n",
    "\n",
    "    def forecast(self, steps: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate ensemble forecast\n",
    "\n",
    "        Args:\n",
    "            steps (int): Number of steps to forecast\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Ensemble forecasted values\n",
    "        \"\"\"\n",
    "\n",
    "        forecasts = [f.forecast(steps) for f in self.forecasters]\n",
    "        return np.mean(forecasts, axis=0)\n",
    "\n",
    "    def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate ensemble performance on test data\n",
    "\n",
    "        Args:\n",
    "            X_test (np.ndarray): Test input features\n",
    "            y_test (np.ndarray): Test target values\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: Evaluation metrics for each forecaster and ensemble\n",
    "        \"\"\"\n",
    "\n",
    "        results: Dict[str, Any] = {\"individual\": {}, \"ensemble\": {}}\n",
    "        for i, forecaster in enumerate(self.forecasters):\n",
    "            results[\"individual\"][f\"Model_{i}\"] = forecaster.evaluate(X_test, y_test)\n",
    "\n",
    "        ensemble_predictions = self.forecast(len(y_test))\n",
    "        results[\"ensemble\"] = {\n",
    "            \"MSE\": mean_squared_error(y_test, ensemble_predictions),\n",
    "            \"MAE\": mean_absolute_error(y_test, ensemble_predictions),\n",
    "            \"MAPE\": mean_absolute_percentage_error(y_test, ensemble_predictions),\n",
    "        }\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(\n",
    "    data: Dict[str, pd.DataFrame],\n",
    "    target_column: str = \"Close\",\n",
    "    test_size: float = 0.2,\n",
    "    sequence_length: int = 30,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Prepare time series data for forecasting\n",
    "\n",
    "    Args:\n",
    "        data (Dict[str, pd.DataFrame]): Dictionary of stock DataFrames\n",
    "        target_column (str, optional): Target column for forecasting. Defaults to \"Close\".\n",
    "        test_size (float, optional): Test data ratio. Defaults to 0.2.\n",
    "        sequence_length (int, optional): Sequence length for LSTM. Defaults to 30.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: Train and test data\n",
    "    \"\"\"\n",
    "\n",
    "    stock_data = list(data.values())[0]\n",
    "    time_series = stock_data[target_column]\n",
    "\n",
    "    X, y = (\n",
    "        DataProcessor.create_lagged_method(time_series, lags=sequence_length).values[\n",
    "            :, :-1\n",
    "        ],\n",
    "        time_series.values[sequence_length:],\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor = DataProcessor(\n",
    "    tickers=[\"AAPL\"], start_date=\"2010-01-01\", end_date=\"2024-01-01\"\n",
    ")\n",
    "\n",
    "stock_data = data_processor.fetch_market_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_data(stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_forecaster = SARIMAForecaster()\n",
    "lstm_forecaster = LSTMForecaster(input_size=X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = ForecastEnsemble([sarima_forecaster, lstm_forecaster])\n",
    "ensemble.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_steps = len(y_test)\n",
    "ensemble_forecast = ensemble.forecast(forecast_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = ensemble.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(stock_data.index[-len(y_test) :], y_test, label=\"Actual\")\n",
    "plt.plot(stock_data.index[-len(y_test) :], ensemble_forecast, label=\"Ensemble Forecast\")\n",
    "plt.title(\"Stock Price Forecast\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
